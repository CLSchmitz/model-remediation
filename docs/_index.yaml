# Copyright 2020 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

book_path: /responsible_ai/_book.yaml
project_path: /responsible_ai/_project.yaml
title: Model Remediation
landing_page:
  custom_css_path: /site-assets/css/style.css
  nav: left
  meta_tags:
  - name: description
    content: >
      MinDiff techniques for model remediation
  rows:
  - classname: devsite-landing-row-100
  - heading: What is Model Remediation?
    options:
    - description-100
    items:
    - description: >
        <p>
        Once you’ve performed <a
        href="../fairness_indicators/guide/guidance">sliced evaluation</a> of a machine
        learning model’s performance, you might notice that your model is underperforming across
        certain slices of data. This type of unequal performance can sometimes lead to unfair and
        potentially harmful outcomes for vulnerable subsets of the population. Generally, there are
        three primary types of technical interventions for addressing bias concerns:
        <ul style="padding-left: 20px">
          <li>
              <strong>Changing the input data:</strong> Collecting more data, generating synthetic
              data, adjusting the weights and sampling rates of different slices, etc.<sup>1</sup>
          </li>
          <li>
              <strong>Intervening on the model:</strong> Changing the model itself by introducing or
              altering model objectives, adding constraints, etc.<sup>2</sup>
          </li>
          <li>
              <strong>Post-processing the results:</strong> Modifying the outputs of the model or
              the interpretation of the outputs to improve performance across metrics.<sup>3</sup>
          </li>
        </ul>
        </p>

    - code_block: |
          <pre class = "prettyprint">
          from tensorflow_model_remediation import min_diff
          import tensorflow as tf

          # Start by defining a Keras model.
          original_model = ...

          # Set the MinDiff weight and choose a loss.
          min_diff_loss = min_diff.losses.MMDLoss()
          min_diff_weight = 1.0  # Hyperparamater to be tuned.

          # Create a MinDiff model.
          min_diff_model = min_diff.keras.MinDiffModel(
          original_model, min_diff_loss, min_diff_weight)

          # Compile the MinDiff model normally.
          min_diff_model.compile(...)

          # Create a MinDiff Dataset and train the min_diff_model.
          min_diff_model.fit(min_diff_dataset, ...)
          </pre>

  - classname: devsite-landing-row-100
  - heading: What is MinDiff?
    options:
    - description-100
    items:
    - classname: devsite-landing-row-100
      description: >
        <p>MinDiff is a model remediation technique that seeks to equalize two distributions. In
        practice, it can be used to balance error rates across different slices of your data by
        penalizing distributional differences.</p>
        <p>Typically, one applies MinDiff when trying to minimize the difference in either false
        positive rate (FPR) or false negative rate (FNR) between a slice of data belonging to a
        sensitive class and a better performing slice. For in-depth discussion of fairness metrics,
        review the literature on this subject.<sup>4 5 6</sup></p>
        </p>

  - classname: devsite-landing-row-100
  - heading: How does MinDiff work?
    options:
    - description-100
    items:
    - classname: devsite-landing-row-100
      description: >
        <p>Given two sets of examples from our dataset, MinDiff penalizes the model during training
        for differences in the distribution of scores between the two sets. The less distinguishable
        the two sets are based on prediction scores, the smaller the penalty that will be
        applied.</p>
        <figure>
            <img src="images/mindiff_graphs.svg">
        </figure>
        <p>The penalty is applied by adding a component to the loss with which the model is training.
        It can be thought of as a measurement of the difference in distribution of model
        predictions. As the model trains, it will try to minimize the penalty by bringing the
        distributions closer together, as in the above graph.</p>
        <p>Applying MinDiff may come with tradeoffs with respect to performance on the original
        task. In practice, we have often found MinDiff to be effective while not deteriorating
        performance beyond product needs, but this will be application dependent and the decision
        should be made deliberately by the product owner. For examples showing how to implement
        MinDiff, see <a
        href="./min_diff/tutorials/min_diff_keras">our notebook tutorial</a>.</p>

        <sup>1</sup>Zhang, G., Bai, B., Zhang, J., Bai, K., Zhu, C., Zhao, T. (2020). <a
        href="https://arxiv.org/abs/2004.14088">Demographics Should Not Be the Reason of Toxicity:
        Mitigating Discrimination in Text Classifications with Instance Weighting.</a><br>
        <sup>2</sup>Prost, F., Qian H., Chen, Q., Chi, E., Chen, J., Beutel, A. (2019). <a
        href="https://arxiv.org/abs/1910.11779">Toward a better trade-off between performance and
        fairness with kernel-based distribution matching.</a><br>
        <sup>3</sup>Alabdulmohsin, I. (2020). <a href="https://arxiv.org/abs/2005.14621">Fair
        Classification via Unconstrained Optimization.</a><br>
        <sup>4</sup>Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R. (2011). <a
        href="https://arxiv.org/abs/1104.3913">Fairness Through Awareness.</a><br>
        <sup>5</sup>Hardt, M., Price, E., Srebro, N. (2016). <a
        href="https://arxiv.org/abs/1610.02413">Equality of Opportunity in Supervised
        Learning.</a><br>
        <sup>6</sup>Chouldechova, A. (2016). <a
        href="https://arxiv.org/abs/1610.07524">Fair prediction with disparate impact: A study of
        bias in recidivism prediction instruments.</a>

  - classname: devsite-landing-row-100
    items:
    - description: >
        <h3>Resources</h3>

  - classname: devsite-landing-row-cards
    items:
    - heading: "See MinDiff applied on a text classification model"
      image_path: /resources/images/tf-logo-card-16x9.png
      path: /responsible_ai/model_remediation/min_diff/tutorials/min_diff_keras
      buttons:
      - label: "Check out the notebook"
        path: /responsible_ai/model_remediation/min_diff/tutorials/min_diff_keras

    - heading: "MinDiff on the TensorFlow blog"
      image_path: /resources/images/tf-logo-card-16x9.png
      path: https://blog.tensorflow.org/2020/11/applying-mindiff-to-improve-model.html
      buttons:
      - label: "Read on the TensorFlow blog"
        path: https://blog.tensorflow.org/2020/11/applying-mindiff-to-improve-model.html

    - heading: "Model Remediation Library on GitHub"
      image_path: /resources/images/github-card-16x9.png
      path: https://github.com/tensorflow/model-remediation
      buttons:
      - label: "View on GitHub"
        path: https://github.com/tensorflow/model-remediation
