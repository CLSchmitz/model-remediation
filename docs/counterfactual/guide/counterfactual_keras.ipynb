{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpbnPF_MEv4h"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mpJkgaM6rqGl"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FCDIaf8wsll"
      },
      "source": [
        "# Model Remediation Case Study\n",
        "\n",
        "\u003cdiv class=\"devsite-table-wrapper\"\u003e\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\u003ca target=\"_blank\" href=\"https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_keras\"\u003e\n",
        "  \u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "\u003c/td\u003e\n",
        "\u003ctd\u003e\n",
        "  \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/model-remediation/blob/master/docs/counterfactual/guide/counterfactual_keras.ipynb\"\u003e\n",
        "  \u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"\u003eRun in Google Colab\u003c/a\u003e\n",
        "\u003c/td\u003e\n",
        "\u003ctd\u003e\n",
        "  \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/model-remediation/blob/master/docs/counterfactual/guide/counterfactual_keras.ipynb\"\u003e\n",
        "  \u003cimg width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"\u003eView source on GitHub\u003c/a\u003e\n",
        "\u003c/td\u003e\n",
        "\u003ctd\u003e\n",
        "  \u003ca target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/model-remediation/docs/counterfactual/guide/counterfactual_keras.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "\u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://tfhub.dev/\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" /\u003eSee TF Hub model\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e\u003c/div\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2iO68j3sC7g"
      },
      "source": [
        "In this notebook, you’ll train a text classifier to identify written content that could be considered toxic or harmful. Counterfactual can be used to improve classifier robustness, by identifying and mitigating correlations between identity terms and the toxicity score. For example, this type of correlation was seen in the Perspective API, which uses machine learning to identify toxic comments. Perspective API takes comment text as input and returns a \"score\" from 0 to 1 that indicates the probability that the comment is similar to toxic comments it's seen in the past. A score of 0 signifies 0% probability that the comment is toxic, a score of 1 indicates 100% probability that the comment is toxic, and a score of 0.5 denotes a 50% probability that the comment is toxic (i.e., that the model is not sure).\n",
        "\n",
        "After the initial launch of Perspective API, external users discovered a positive correlation between identity terms containing information on race or sexual orientation and toxicity score. For example, the phrase \"I am a gay black woman\" received a toxicity score of 0.87. In this case, the identity terms were not being used pejoratively, so this example was classified incorrectly. Counterfactual can be used to remediate this incorrect correlation and improve classifier precision in similar cases.\n",
        "\n",
        "Specifically, you will:\n",
        "\n",
        "1.   Build a baseline model and measure its performance on text containing references to gender groups.\n",
        "2.   Build a counterfactual dataset and evaluate the model’s performance on flip rate and flip count to determine if Counterfactual should be applied. \n",
        "3.   Train with the Counterfactual technique to avoid unintended correlation between model output and sensitive identity terms.\n",
        "4.   Evaluate the new model’s performance on the flip rate and flip count.\n",
        "\n",
        "This tutorial demonstrates usage of the Counterfactual technique with a very minimal workflow, not to lay out a principled approach to fairness in machine learning. The Counterfactual technique is one tool in the broader [Responsible AI Toolkit](https://www.tensorflow.org/responsible_ai). You still want to evaluate the performance of your model across overall errors rates, as demonstrated in the MinDiff tutorial. You also don’t address potential shortcomings in the dataset, nor tune our configurations. In a production setting, you would want to approach each of these  fairness concerns with rigor. For more information on evaluating for fairness, see the guide for [Fairness Indicators](https://www.tensorflow.org/responsible_ai/fairness_indicators/guide).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exTI9lyLsZjm"
      },
      "source": [
        "## Setup\n",
        "\n",
        "You begin by installing Fairness Indicators and TensorFlow Model Remediation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1af32h2sSqC"
      },
      "outputs": [],
      "source": [
        "#@title Installs\n",
        "!pip install --upgrade tensorflow-model-remediation\n",
        "!pip install --upgrade fairness-indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bifRLJaCsVlW"
      },
      "source": [
        "Import all necessary components, including Counterfactual and Fairness Indicators for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYLW8UIsIMrE"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "import zipfile\n",
        " \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_model_analysis as tfma\n",
        "from google.protobuf import text_format\n",
        "\n",
        " \n",
        "# Import Counterfactuals.\n",
        "from tensorflow_model_remediation import counterfactual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTDvp292s1ZP"
      },
      "source": [
        "You use a [utility function](#Utility Functions) called `download_and_process_civil_comments_data` to download the preprocessed data and prepare the labels to match the model’s output shape. The function also downloads the data as TFRecords to make later evaluation quicker. Alternatively, you can convert the Pandas DataFrame into TFRecords with any available utility conversion function.\n",
        "\n",
        "First define a few useful constants and train the model on the ’comment_text’ feature, with the target label as ’toxicity’. Note that the batch size here is chosen arbitrarily, but in a production setting you would need to tune it for best performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phRmJUFus8Wf"
      },
      "outputs": [],
      "source": [
        "TEXT_FEATURE = 'comment_text'\n",
        "LABEL = 'toxicity'\n",
        "BATCH_SIZE = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYpmPKx7svUg"
      },
      "outputs": [],
      "source": [
        "#@title Utility Functions\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "def download_and_process_civil_comments_data():\n",
        "  \"\"\"Download and process the civil comments dataset into a Pandas DataFrame.\"\"\"\n",
        "\n",
        "  # Download data.\n",
        "  toxicity_data_url = 'https://storage.googleapis.com/civil_comments_dataset/'\n",
        "  train_csv_file = tf.keras.utils.get_file(\n",
        "      'train_df_processed.csv', toxicity_data_url + 'train_df_processed.csv')\n",
        "  validate_csv_file = tf.keras.utils.get_file(\n",
        "      'validate_df_processed.csv',\n",
        "      toxicity_data_url + 'validate_df_processed.csv')\n",
        "\n",
        "  # Get validation data as TFRecords.\n",
        "  validate_tfrecord_file = tf.keras.utils.get_file(\n",
        "      'validate_tf_processed.tfrecord',\n",
        "      toxicity_data_url + 'validate_tf_processed.tfrecord')\n",
        "\n",
        "  # Read data into Pandas DataFrame.\n",
        "  data_train = pd.read_csv(train_csv_file)\n",
        "  data_validate = pd.read_csv(validate_csv_file)\n",
        "\n",
        "  # Fix type interpretation.\n",
        "  data_train[TEXT_FEATURE] = data_train[TEXT_FEATURE].astype(str)\n",
        "  data_validate[TEXT_FEATURE] = data_validate[TEXT_FEATURE].astype(str)\n",
        "\n",
        "  # Shape labels to match output.\n",
        "  labels_train = data_train[LABEL].values.reshape(-1, 1) * 1.0\n",
        "  labels_validate = data_validate[LABEL].values.reshape(-1, 1) * 1.0\n",
        "\n",
        "  return data_train, data_validate, validate_tfrecord_file, labels_train, labels_validate\n",
        "\n",
        "data_train, data_validate, validate_tfrecord_file, labels_train, labels_validate = download_and_process_civil_comments_data()\n",
        "\n",
        "def _create_embedding_layer(hub_url):\n",
        "  return hub.KerasLayer(\n",
        "      hub_url, output_shape=[128], input_shape=[], dtype=tf.string)\n",
        "  \n",
        "def create_keras_sequential_model(\n",
        "    hub_url='https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1',\n",
        "    cnn_filter_sizes=[128, 128, 128],\n",
        "    cnn_kernel_sizes=[5, 5, 5],\n",
        "    cnn_pooling_sizes=[5, 5, 40]):\n",
        "  \"\"\"Create baseline keras sequential model.\"\"\"\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Embedding layer.\n",
        "  hub_layer = _create_embedding_layer(hub_url)\n",
        "  model.add(hub_layer)\n",
        "  model.add(tf.keras.layers.Reshape((1, 128))) # why?\n",
        "\n",
        "  # Convolution layers.\n",
        "  for filter_size, kernel_size, pool_size in zip(cnn_filter_sizes,\n",
        "                                                 cnn_kernel_sizes,\n",
        "                                                 cnn_pooling_sizes):\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv1D(\n",
        "            filter_size, kernel_size, activation='relu', padding='same'))\n",
        "    model.add(tf.keras.layers.MaxPooling1D(pool_size, padding='same'))\n",
        "\n",
        "  # Flatten, fully connected, and output layers.\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpi5Buu-tIX_"
      },
      "source": [
        "## Define and train the baseline model\n",
        "\n",
        "Now, you’ll use [Counterfactual](https://arxiv.org/abs/1809.10610) to mitigate any unintended correlation between the toxicity score and identity terms. For example, if the dataset includes a text comment stating something like “she is toxic”, it could push our model to be slightly biased against women. Counterfactuals adjusts the loss between an original and counterfactual example, which will compensate for the gender bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hw5HdppwuBs"
      },
      "outputs": [],
      "source": [
        "use_pretrained_model = True #@param {type:\"boolean\"}\n",
        "\n",
        "if use_pretrained_model:\n",
        " URL = 'https://storage.googleapis.com/civil_comments_model/baseline_model.zip'\n",
        " ZIPPATH = 'baseline_model.zip'\n",
        " DIRPATH = '/tmp/baseline_model'\n",
        " r = requests.get(URL, allow_redirects=True)\n",
        " open(ZIPPATH, 'wb').write(r.content)\n",
        " \n",
        " with zipfile.ZipFile(ZIPPATH, 'r') as zip_ref:\n",
        "   zip_ref.extractall('/')\n",
        " baseline_model = tf.keras.models.load_model(\n",
        "     DIRPATH, custom_objects={'KerasLayer' : hub.KerasLayer})\n",
        " \n",
        "else:\n",
        " optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        " loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        " \n",
        " baseline_model = (\n",
        "   create_keras_sequential_model())\n",
        " baseline_model.compile(optimizer=optimizer, loss=loss,      \n",
        "                        metrics=['accuracy'])\n",
        " \n",
        " baseline_model.fit(x=data_train[TEXT_FEATURE],\n",
        "                    y=labels_train, batch_size=BATCH_SIZE,\n",
        "                    epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhhjuPAgveyS"
      },
      "source": [
        "To evaluate the original model's performance using Fairness Indicators you will need to save the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIrITNYgvqLA"
      },
      "outputs": [],
      "source": [
        "#@title Save Model\n",
        "base_dir = tempfile.mkdtemp(prefix='saved_models')\n",
        "baseline_model_location = os.path.join(base_dir, 'model_export_baseline')\n",
        "baseline_model.save(baseline_model_location, save_format='tf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6E_cWSxwjzH"
      },
      "source": [
        "## Determine if Counterfactual is needed\n",
        "\n",
        "For the purpose of this example, you will check to see if this model is incorrectly correlating gender terms to the toxicity of a sentence. Similar to the scenario that occurred in the Perspective API, this may occur if the dataset includes a text comment stating something like “she is toxic”. Counterfactuals can be used to adjust the loss between an original and counterfactual example, which will compensate for the gender bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0T1SLKbwzAr"
      },
      "source": [
        "###Preparing the Counterfactual Dataset\n",
        "\n",
        "To use Counterfactual, you will first need to create a corresponding counterfactual dataset. This dataset should be in a `tf.data.Dataset` and include only the original x values that include a different counterfactual where (`original_x`, `counterfactual_x`, `counterfactual_sample_weight`). Note that the number of datapoints in your dataset that require a counterfactual will be small. Repeating values within your counterfactual dataset is expected to match the shape of the original dataset.\n",
        "\n",
        "To understand your options for producing a counterfactual dataset, see the Creating a Custom Counterfactual Dataset Colab.  \n",
        "\n",
        "For this situation, you will remove a list of gender specific terms using build_counterfactual_dataset. Note that the default function within build_counterfactual_dataset uses [tf.strings.regex_replace](https://www.tensorflow.org/api_docs/python/tf/strings/regex_replace), which could remove more than you intended. For example, passing only the word “he” would change words like “the” into “t”; “there” into “tre”. Additionally since you’re passing the list of words through `create_capitalization_regex_list` to include both upper and lower case letters if the terms start at the beginning of a sentence.\n",
        "\n",
        "In this list, we include non-pejorative terms only because pejorative terms should have a different toxicity score. Requiring equal predictions across examples with pejorative terms can accidentally harm the more vulnerable group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWMPnTMLgwtI"
      },
      "outputs": [],
      "source": [
        "#@title Create Counterfactual Inputs\n",
        "\n",
        "sensitive_terms_to_remove = [\n",
        "  'aunt', 'boy', 'brother', 'dad', 'daughter', 'father', 'female', 'gay',\n",
        "  'girl', 'grandma', 'grandpa', 'grandson', 'grannie', 'granny', 'he',\n",
        "  'heir', 'her', 'him', 'his', 'hubbies', 'hubby', 'husband', 'king',\n",
        "  'knight', 'lad', 'ladies', 'lady', 'lesbian', 'lord', 'man', 'male',\n",
        "  'mom', 'mother', 'mum', 'nephew', 'niece', 'prince', 'princess',\n",
        "  'queen', 'queens', 'she', 'sister', 'son', 'uncle', 'waiter',\n",
        "  'waitress', 'wife', 'wives', 'woman', 'women'\n",
        "]\n",
        "\n",
        "def create_capitalization_regex_list():\n",
        "  return_list = []\n",
        "  for term in sensitive_terms_to_remove:\n",
        "    return_list.append(f'\\b{term[0].upper()}{term[0].lower()}{term[1:]}\\b')\n",
        "  return return_list\n",
        "\n",
        "# Convert the Pandas DataFrame to a TF Dataset\n",
        "dataset_train_main = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_train[TEXT_FEATURE].values, labels_train)).batch(BATCH_SIZE)\n",
        "\n",
        "counterfactual_data = counterfactual.keras.utils.build_counterfactual_dataset(\n",
        "    original_dataset=dataset_train_main,\n",
        "    sensitive_terms_to_remove=create_capitalization_regex_list())\n",
        "\n",
        "counterfactual_packed_input = counterfactual.keras.utils.pack_counterfactual_data(\n",
        "  dataset_train_main,\n",
        "  counterfactual_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF28-KpvxDvu"
      },
      "source": [
        "Now that you created counterfactual dataset with terms that have been replaced, you can pack the original dataset and counterfactual datasets together that can be passed to the CounterfactualModel. Note that `build_counterfactual_dataset` returns only the original values with the sensitive terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_6at354kEZO"
      },
      "source": [
        "## Calculate the Flip Rate and Flip Count\n",
        "Next run Fairness Indicators. As a reminder, you’re just going to calculate the flip rate and flip count to see if the model is incorrectly associating gender identity terms with toxicity.  A ‘flip’ is defined as a classifier giving a different decision when the identity term in the example changes. Flip count measures the number of times the classifier gives a different decision if the identity term in a given example were changed. Flip rate measures the probability that the classifier gives a different decision if the identity term in a given example were changed.\n",
        "\n",
        "To compute model performance, the utility function makes a few convenient choices for metrics, slices, and classifier thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdP7vZF9lGTf"
      },
      "outputs": [],
      "source": [
        "#@title Run Model Analysis\n",
        "\n",
        "def get_eval_results(model_location,\n",
        "                     eval_result_path,\n",
        "                     validate_tfrecord_file,\n",
        "                     slice_selection='gender',\n",
        "                     compute_confidence_intervals=True):\n",
        "  \"\"\"Get Fairness Indicators eval results.\"\"\"\n",
        "  # Define slices that you want the evaluation to run on.\n",
        "  eval_config = text_format.Parse(\n",
        "      \"\"\"\n",
        "    model_specs {\n",
        "     label_key: '%s'\n",
        "   }\n",
        "   metrics_specs {\n",
        "     metrics {class_name: \"AUC\"}\n",
        "     metrics {class_name: \"ExampleCount\"}\n",
        "     metrics {class_name: \"Accuracy\"}\n",
        "     metrics {\n",
        "        class_name: \"FairnessIndicators\"\n",
        "     }\n",
        "     metrics {\n",
        "        class_name: \"FlipRate\"\n",
        "        config: '{ \"counterfactual_prediction_key\": \"toxicity\", '\n",
        "                  '\"example_id_key\": 1 }'\n",
        "     }\n",
        "   }\n",
        "   slicing_specs {\n",
        "     feature_keys: '%s'\n",
        "   }\n",
        "   slicing_specs {}\n",
        "   options {\n",
        "       compute_confidence_intervals { value: %s }\n",
        "       disabled_outputs{values: \"analysis\"}\n",
        "   }\n",
        "   \"\"\" % (LABEL,\n",
        "          slice_selection, 'true' if compute_confidence_intervals else 'false'),\n",
        "      tfma.EvalConfig())\n",
        "  \n",
        "  eval_shared_model = tfma.default_eval_shared_model(\n",
        "      eval_saved_model_path=model_location, tags=[tf.saved_model.SERVING])\n",
        "\n",
        "  return tfma.run_model_analysis(\n",
        "      eval_shared_model=eval_shared_model,\n",
        "      data_location=validate_tfrecord_file,\n",
        "      eval_config=eval_config,\n",
        "      output_path=eval_result_path)\n",
        "  \n",
        "base_dir = tempfile.mkdtemp(prefix='eval')\n",
        "eval_dir = os.path.join(base_dir, 'tfma_eval_result_no_cf')\n",
        "base_eval_result = get_eval_results(\n",
        "    baseline_model_location,\n",
        "    eval_dir,\n",
        "    validate_tfrecord_file,\n",
        "    slice_selection='gender')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGa07pJcl9n3"
      },
      "outputs": [],
      "source": [
        "#@title Render Evaluation Results\n",
        "tfma.addons.fairness.view.widget_view.render_fairness_indicator(\n",
        "    eval_result=base_eval_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24x47msvskkj"
      },
      "source": [
        "Let’s look at the evaluation results above. Once Fairness Indicators render select “flip_rate/overall”, which will be filtered down to gender. You’ll notice that there are four gender types within this dataset: “female”, “male”, “transgender”, and “other_gender”. For this Colab we will focus on “female” and “male” since example count is low within this dataset for the other gender. \n",
        "\n",
        "You’ll notice that the flip rate for females is about 13% and male about 14%, which are both higher than the overall dataset of 7%. Additionally, comparing these numbers to the total count found within “flip_rate/nagative_to_positive” and “flip_rate/positive_to_nagative” we can see that the likelihood for both male and female to incorrectly flip from negative to positive is high. In other words our model is more likely to predict the content of our text is toxic if it includes gender terms.\n",
        "\n",
        "You'll not use Counterfactual remediation to try to reduce the flip rate and count for gender related terms in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whjVKbCgzI3U"
      },
      "source": [
        "###Training and Evaluating the Counterfactual Model\n",
        "\n",
        "To train with Counterfactual, simply take the original model and wrap it in a CounterfactualModel with a corresponding `loss` and `loss_weight`. You will start out using 1.5 as the default `loss_weight`, but this is a parameter that can be tuned for your use case, since it depends on your model and product requirements.  \n",
        "\n",
        "Next compile the model normally (using the regular non-Counterfactual loss) and fit to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6j8cCHV2gVw"
      },
      "outputs": [],
      "source": [
        "#@title Train Model\n",
        "\n",
        "counterfactual_weight = 1.5 #@param {type:\"number\"}\n",
        " \n",
        "base_dir = tempfile.mkdtemp(prefix='saved_models')\n",
        "counterfactual_model_location = os.path.join(\n",
        "    base_dir, 'model_export_counterfactual')\n",
        " \n",
        "counterfactual_model = counterfactual.keras.CounterfactualModel(\n",
        "    baseline_model)\n",
        " \n",
        "# Compile the model normally after wrapping the original model.\n",
        "# Note that this means we use the baseline's model's loss here.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "counterfactual_model.compile(optimizer=optimizer, loss=loss, \n",
        "                             metrics=['accuracy'])\n",
        " \n",
        "counterfactual_model.fit(counterfactual_packed_input,\n",
        "                         epochs=10)\n",
        " \n",
        "counterfactual_model.save_original_model(counterfactual_model_location,\n",
        "                                         save_format='tf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu6HTV_Uzc98"
      },
      "source": [
        "Next evaluate the results with the Counterfactual model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxAEZic8F3ab"
      },
      "outputs": [],
      "source": [
        "def get_eval_results_counterfactual(\n",
        "                     baseline_model_location,\n",
        "                     counterfactual_model_location,\n",
        "                     eval_result_path,\n",
        "                     validate_tfrecord_file,\n",
        "                     slice_selection='gender',\n",
        "                     compute_confidence_intervals=True):\n",
        "  \"\"\"Get Fairness Indicators eval results.\"\"\"\n",
        "  # Define slices that you want the evaluation to run on.\n",
        "  eval_config = text_format.Parse(\n",
        "      \"\"\"\n",
        "    model_specs {\n",
        "     name: 'original'\n",
        "     label_key: '%s'\n",
        "   }\n",
        "   model_specs {\n",
        "     name: 'counterfactual'\n",
        "     label_key: '%s'\n",
        "     is_baseline: true\n",
        "   }\n",
        "   metrics_specs {\n",
        "     metrics {class_name: \"AUC\"}\n",
        "     metrics {class_name: \"ExampleCount\"}\n",
        "     metrics {class_name: \"Accuracy\"}\n",
        "     metrics {\n",
        "        class_name: \"FairnessIndicators\"\n",
        "     }\n",
        "     metrics {\n",
        "        class_name: \"FlipRate\"\n",
        "        config: '{ \"example_ids_count\": 0 }'\n",
        "     }\n",
        "     metrics {\n",
        "        class_name: \"FlipCount\"\n",
        "        config: '{ \"example_ids_count\": 0 }'\n",
        "     }\n",
        "   }\n",
        "   slicing_specs {\n",
        "     feature_keys: '%s'\n",
        "   }\n",
        "   slicing_specs {}\n",
        "   options {\n",
        "       disabled_outputs{ values: \"analysis\"}\n",
        "   }\n",
        "   \"\"\" % (LABEL, LABEL, slice_selection,),\n",
        "      tfma.EvalConfig())\n",
        "\n",
        "  eval_shared_models = [\n",
        "      tfma.default_eval_shared_model(\n",
        "          model_name='original',\n",
        "          eval_saved_model_path=baseline_model_location,\n",
        "          eval_config=eval_config,\n",
        "          tags=[tf.saved_model.SERVING]),\n",
        "      tfma.default_eval_shared_model(\n",
        "          model_name='counterfactual',\n",
        "          eval_saved_model_path=counterfactual_model_location,\n",
        "          eval_config=eval_config,\n",
        "          tags=[tf.saved_model.SERVING]),\n",
        "    ]\n",
        "  \n",
        "  return tfma.run_model_analysis(\n",
        "      eval_shared_model=eval_shared_models,\n",
        "      data_location=validate_tfrecord_file,\n",
        "      eval_config=eval_config,\n",
        "      output_path=eval_result_path)\n",
        " \n",
        "counterfactual_eval_dir = os.path.join(base_dir, 'tfma_eval_result_cf') \n",
        "counterfactual_eval_result = get_eval_results_counterfactual(\n",
        "  baseline_model_location,\n",
        "  counterfactual_model_location,\n",
        "  counterfactual_eval_dir,\n",
        "  validate_tfrecord_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moWNsGiR3bOI"
      },
      "outputs": [],
      "source": [
        "#@title Render Evaluation Results\n",
        " \n",
        "counterfactual_model_comparison_results = {\n",
        "    'base_model': base_eval_result,\n",
        "    'counterfactual': counterfactual_eval_result.get_results()[0],\n",
        "}\n",
        "tfma.addons.fairness.view.widget_view.render_fairness_indicator(\n",
        "    multi_eval_results=counterfactual_model_comparison_results\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJaw05-OzhrD"
      },
      "source": [
        "Now you’ll evaluate the Counterfactual model by passing both the original and counterfactual into Fairness Indicators together to get a side-by-side comparison. Once again select “flip_rate/overall” and compare the results for female and male  between the two models. You should notice that the overall flip rate for overall, female, and male have all decreased by about 85% whilst female at approximately 1.5% and male at approximately 2%. \n",
        "\n",
        "Additionally, reviewing “flip_rate/nagative_to_positive” and “flip_rate/positive_to_nagative” again you’ll notice that our model is still more likely to flip gender related content to toxic, but the total count has decreased by over 90%\n",
        "\n",
        "\n",
        "You’ll notice that the flip rate for females is about 13% and male about 14%, which are both higher than the overall dataset of 7%. Additionally, comparing these numbers to the total count found within “flip_rate/nagative_to_positive” and “flip_rate/positive_to_nagative” we can see that the likelihood for both male and female to incorrectly flip from negative to positive is high. In other words our model is more likely to predict the content of our text is toxic if it includes gender terms\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "counterfactual_keras.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
